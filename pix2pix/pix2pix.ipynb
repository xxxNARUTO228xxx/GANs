{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"pix2pix.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"AMrDA2PDK0S6"},"source":["# pix2pix\n"]},{"cell_type":"code","metadata":{"id":"0vJrOIltK0S8"},"source":["import torch\n","from torch import nn\n","import numpy as np\n","import os\n","import PIL\n","from PIL import Image\n","import albumentations as A\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from albumentations.pytorch import ToTensorV2\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","device = torch.device(\"cuda:1\" if (torch.cuda.is_available()) else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TAnConbWK0S-"},"source":["## Some utils"]},{"cell_type":"code","metadata":{"id":"F3MoJiblK0S_"},"source":["from torchvision.utils import save_image\n","\n","def save_some_examples(gen, val_loader, epoch, path):\n","    x, y = next(iter(val_loader))\n","    x, y = x.to(device), y.to(device)\n","    gen.eval()\n","    with torch.no_grad():\n","        y_fake = gen(x)\n","        y_fake = y_fake * 0.5 + 0.5  # remove normalization\n","        save_image(y_fake, path + f\"y_gen_{epoch}.png\")\n","        save_image(x * 0.5 + 0.5, path + f\"input_{epoch}.png\")\n","        if epoch == 1:\n","            save_image(y * 0.5 + 0.5, path + f\"label_{epoch}.png\")\n","    gen.train()\n","\n","\n","def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    checkpoint = {\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(checkpoint_file, map_location=device)\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"31vjcELvK0TB"},"source":["class AnimeDataset(Dataset):\n","    def __init__(self, root_dir, img_size=512): # im_size = real img size, but im_size for model is 256\n","        self.img_size = img_size\n","        self.root_dir = root_dir\n","        self.list_files = os.listdir(self.root_dir)\n","        self.transform_input = A.Compose([\n","                A.Resize(width=256, height=256),\n","                A.HorizontalFlip(p=0.5),\n","                A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n","                ToTensorV2(),\n","            ])\n","        self.transform_target = A.Compose([\n","                A.Resize(width=256, height=256),\n","                A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n","                #A.ColorJitter(p=0.2),\n","                ToTensorV2(),\n","            ])\n","\n","    def __len__(self):\n","        return len(self.list_files)\n","\n","    def __getitem__(self, index):\n","        img_file = self.list_files[index]\n","        img_path = os.path.join(self.root_dir, img_file)\n","        image = np.array(Image.open(img_path))\n","        \n","        input_image = image[:, self.img_size:, :]   # change if it's needed, im my case I had label 1st and input 2nd\n","        target_image = image[:, :self.img_size, :]\n","        \n","        input_image = self.transform_input(image=input_image)[\"image\"]\n","        target_image = self.transform_target(image=target_image)[\"image\"]\n","        \n","        return input_image, target_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SX6pC-JnK0TC"},"source":["## Descriminator architecture\n","Where Ck = Convolution-BatchNorm-ReLU layer\n","with k filters\n","\n","The 70 Ã— 70 PatchGAN discriminator architecture is:\n","C64-C128-C256-C512"]},{"cell_type":"code","metadata":{"id":"T8F9YQFvK0TC"},"source":["class CBlock(nn.Module):\n","    def __init__(self, in_ch, out_ch, stride=2):\n","        super(CBlock, self).__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, 4, stride, 1, bias=False),\n","            nn.BatchNorm2d(out_ch),\n","            nn.LeakyReLU(0.2),\n","        )\n","        \n","    def forward(self, x):\n","        return self.block(x)\n","    \n","    \n","class Desc(nn.Module):\n","    def __init__(self, d=64):\n","        super(Desc, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(6, d, 4, 2, 1),   # CHANGE IN_CHANNELS TO 5 IF IT'S COLORIZATION\n","            nn.LeakyReLU(0.2),\n","            CBlock(d, d*2),\n","            CBlock(d*2, d*4),\n","            CBlock(d*4, d*8, 1),\n","            nn.Conv2d(d*8, 1, 4, 1, 1)\n","        )\n","\n","    def forward(self, x, y):\n","        xy = torch.cat([x,y], dim=1)\n","        return self.model(xy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vVCaQoyIK0TD"},"source":["## Generator architecture\n","U-net generator"]},{"cell_type":"code","metadata":{"id":"8qEN-w9yK0TD"},"source":["class CDBlock(nn.Module):\n","    def __init__(self, in_ch, out_ch, stride=2, use_dropout=False):\n","        super(CDBlock, self).__init__()\n","        self.conv = nn.ConvTranspose2d(in_ch, out_ch, 4, stride, 1, bias=False)\n","        self.batch = nn.BatchNorm2d(out_ch)\n","        self.drop = nn.Dropout(0.5)\n","        self.act = nn.ReLU()\n","        self.use_dropout = use_dropout\n","        \n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.batch(x)\n","        if self.use_dropout:\n","            x = self.drop(x)\n","        return self.act(x)\n","\n","    \n","class Gen(nn.Module):\n","    def __init__(self, d=64):\n","        super(Gen, self).__init__()\n","        # encoder\n","        self.down1 = nn.Sequential(\n","            nn.Conv2d(3, d, 4, 2, 1), # !!! in 2 channels if it's colorization\n","            nn.LeakyReLU(0.2),\n","        )\n","        self.down2 = CBlock(d, d*2)\n","        self.down3 = CBlock(d*2, d*4)\n","        self.down4 = CBlock(d*4, d*8)\n","        self.down5 = CBlock(d*8, d*8)\n","        self.down6 = CBlock(d*8, d*8)\n","        self.down7 = CBlock(d*8, d*8)\n","        self.down8 = nn.Sequential(\n","            nn.Conv2d(d*8, d*8, 4, 2, 1), nn.ReLU()\n","        )\n","        # decoder\n","        self.up1 = CDBlock(d*8, d*8, use_dropout=True)\n","        self.up2 = CDBlock(d*8*2, d*8, use_dropout=True)\n","        self.up3 = CDBlock(d*8*2, d*8, use_dropout=True)\n","        self.up4 = CDBlock(d*8*2, d*8)\n","        self.up5 = CDBlock(d*8*2, d*4)\n","        self.up6 = CDBlock(d*4*2, d*2)\n","        self.up7 = CDBlock(d*2*2, d)\n","        self.up8 = nn.ConvTranspose2d(d*2, 3, 4, 2, 1) \n","        \n","    def forward(self, x):\n","        d1 = self.down1(x)\n","        d2 = self.down2(d1)\n","        d3 = self.down3(d2)\n","        d4 = self.down4(d3)\n","        d5 = self.down5(d4)\n","        d6 = self.down6(d5)\n","        d7 = self.down7(d6)\n","        d8 = self.down8(d7)\n","        \n","        u1 = self.up1(d8)\n","        u2 = self.up2(torch.cat([u1, d7], 1))\n","        u3 = self.up3(torch.cat([u2, d6], 1))\n","        u4 = self.up4(torch.cat([u3, d5], 1))\n","        u5 = self.up5(torch.cat([u4, d4], 1))\n","        u6 = self.up6(torch.cat([u5, d3], 1))\n","        u7 = self.up7(torch.cat([u6, d2], 1))\n","        u8 = self.up8(torch.cat([u7, d1], 1))\n","        \n","        return torch.tanh(u8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPbyz4_fK0TE"},"source":["## Weights init"]},{"cell_type":"code","metadata":{"id":"MH0i4FKpK0TE","outputId":"a7537b2c-d03a-4d46-f511-21f9959cfd72"},"source":["def weights_init(m, mean=0.0, std=0.02):\n","    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n","        nn.init.normal_(m.weight.data, mean, std)\n","        \n","netG = Gen().to(device)\n","netG.apply(weights_init) \n","netD = Desc().to(device)\n","netD.apply(weights_init) \n","print('weights initialized')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["weights initialized\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vg0qE3_sK0TF"},"source":["# Define params and train"]},{"cell_type":"code","metadata":{"id":"29bHWgUzK0TG"},"source":["im_size = 512\n","dataset = AnimeDataset(\"sketch/\", im_size)\n","loader = DataLoader(dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n","\n","L1_LAMBDA = 100\n","BCE_loss = nn.BCEWithLogitsLoss().to(device)\n","L1_loss = nn.L1Loss().to(device)\n","\n","# Adam optimizer\n","optG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BEGbAeMPK0TG"},"source":["epochs_num = 101\n","for epoch in range(epochs_num):\n","    for img, target in loader:\n","        img, target = img.to(device), target.to(device)\n","        \n","        ### train discriminator ###\n","        # real images\n","        D_real = netD(img, target)\n","        real_loss = BCE_loss(D_real, torch.ones_like(D_real))\n","        \n","        # fake loss\n","        fake = netG(img)\n","        D_fake = netD(img, fake.detach())\n","        fake_loss = BCE_loss(D_fake, torch.zeros_like(D_fake))\n","        \n","        D_loss = (real_loss + fake_loss)/2\n","        netD.zero_grad()\n","        D_loss.backward()\n","        optD.step()\n","        \n","        ### train generator ###\n","        D_fake = netD(img, fake)\n","        G_fake_loss = BCE_loss(D_fake, torch.ones_like(D_fake))\n","        L1 = L1_loss(fake, target) * L1_LAMBDA\n","        \n","        G_loss = G_fake_loss + L1\n","        netG.zero_grad()\n","        G_loss.backward()\n","        optG.step()\n","    \n","    if (epoch%10==0):\n","        save_some_examples(netG, val_loader, epoch)\n","        \n","    if (epoch%10==0):\n","        save_checkpoint(netG, optG, filename=\"netG.pth.tar\")\n","        save_checkpoint(netD, optD, filename=\"netD.pth.tar\")\n","    print(epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zEQlW1LOK0TH"},"source":["# Evaluate\n","\n","Do we really need model.eval?"]},{"cell_type":"code","metadata":{"id":"F7YQPMMDK0TH","outputId":"3e9482af-d1a1-4cfd-cb0f-146723ae0e23"},"source":["load_checkpoint(\"netG.pth.tar\", netG, optG)\n","dataset = AnimeDataset(\"/test/\", im_size)\n","loader = DataLoader(dataset, batch_size=16)\n","#netG.eval()\n","for i in range(5):\n","    x, y = next(iter(loader))\n","    x, y = x.to(device), y.to(device)\n","    with torch.no_grad():\n","        y_fake = netG(x)\n","        y_fake = y_fake * 0.5 + 0.5  # remove normalization\n","        save_image(y_fake, f\"y_gen_{i}.png\")\n","        save_image(x * 0.5 + 0.5, f\"input_{i}.png\")\n","    save_image(y * 0.5 + 0.5, f\"label_{i}.png\")\n","#netG.train()    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["=> Loading checkpoint\n"],"name":"stdout"}]}]}